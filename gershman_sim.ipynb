{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running BERT and GPT2 on Gershman and Tenenbaum's Phrase similarity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from scipy.stats.mstats import rankdata\n",
    "from scipy.stats import sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fragment(myword):\n",
    "    # make sure this is just a word with no spaces\n",
    "    assert(myword == myword.strip()) # no beginning or end space\n",
    "    assert(myword[-1] != '.') # no period\n",
    "    return myword\n",
    "\n",
    "def check_sentence(mysentence):\n",
    "    # make sure we have no extra spaces, and we end with a period\n",
    "    assert(mysentence == mysentence.strip()) # no beginning or end space\n",
    "    assert(mysentence[-1] == '.') # must have a period at the end\n",
    "    return mysentence\n",
    "\n",
    "def check_no_split(tokens):\n",
    "    # tokens should be a 1 x 1 tensor \n",
    "    if tkn.numel()>1:\n",
    "        print(\"Error: word is split in WordPiece\")\n",
    "        assert False\n",
    "    \n",
    "def f_extract(X, tkns, mytype, use_special):\n",
    "    # Input\n",
    "    #  X : [1 x dim x K tensor] which is embedding at each step we want to extract\n",
    "    #  tkns : tokens of phrase we are considering (just for printing)\n",
    "    #  mytype : either \"mean\" (take the average) or \"end\" (take the last word that is not a period)\n",
    "    #  use_speical : are we adding special tokens?\n",
    "    sz = X.size()\n",
    "    assert(sz[0]==1)\n",
    "    if mytype == 'mean':\n",
    "        return torch.mean(X, dim=1)\n",
    "    elif mytype == 'end':        \n",
    "        idx = -3\n",
    "        if (not use_special) or (tokenizer.__class__.__name__ == 'GPT2Tokenizer'):\n",
    "            idx += 1 # there is no extract symbol\n",
    "        if verbose:\n",
    "            print(\"extract embedding for: \" + tokenizer.decode(tkns[:,idx]))\n",
    "        return X[:,idx]\n",
    "    else:\n",
    "        assert False, \"extraction type is undefined\"\n",
    "    \n",
    "def compute_hidden_cosine(query_phrase, list_phrases, use_special=True, extract='mean'):\n",
    "    # Compare respresentations using the top level of a transfomer\n",
    "    # ---\n",
    "    # query_phrase : string to query\n",
    "    # list_phrase : [list of strings to compare against]\n",
    "    # extract : 'mean' for computing mean embedding from top layer, and 'end' for getting embeding from last word\n",
    "    if use_special:\n",
    "        query_tkn = encode_sentence(query_phrase)\n",
    "    else:\n",
    "        query_tkn = encode_sentence_plain(query_phrase)\n",
    "    print(\"Similarity analysis (cosine) from \" + model.__class__.__name__ + \": \")\n",
    "    print(\"'\" + str(query_phrase) + \"'\")\n",
    "    f_get = lambda X, tkns, us : f_extract(X, tkns, extract, us)     \n",
    "    list_S = []\n",
    "    with torch.no_grad():\n",
    "        h_query = model(query_tkn)[0] # 1 x ntkns\n",
    "        h_query = f_get(h_query, query_tkn, use_special)\n",
    "        for phrase in list_phrases:\n",
    "            if use_special:\n",
    "                phrase_tkn = encode_sentence(phrase)\n",
    "            else:\n",
    "                phrase_tkn = encode_sentence_plain(phrase)\n",
    "            h_phrase = model(phrase_tkn)[0] # 1 x ntkns\n",
    "            h_phrase = f_get(h_phrase, phrase_tkn, use_special)\n",
    "            S = F.cosine_similarity(h_query, h_phrase, dim=1)\n",
    "            S = S.item()\n",
    "            list_S.append(S)\n",
    "    sim_phrases = copy(list_S) # original order \n",
    "            \n",
    "    Z = [(x,y) for x,y in sorted(zip(list_phrases,list_S), reverse=True, key=lambda pair: pair[1])]\n",
    "    list_phrases, list_S = zip(*Z) \n",
    "    for ii in range(len(list_S)):\n",
    "        phrase = list_phrases[ii]\n",
    "        cos = str(round(list_S[ii],2))\n",
    "        print(\"  vs. '\" + phrase + \"' : \" + cos)\n",
    "    print(\"\")\n",
    "    return sim_phrases\n",
    "    \n",
    "def predict_mask(sentence, K=3):\n",
    "    sentence_tkn = encode_sentence(sentence)\n",
    "    mask_token_index = torch.nonzero(sentence_tkn == tokenizer.mask_token_id)[:,1]\n",
    "    if mask_token_index.numel()!=1:\n",
    "        assert False, \"wrong number of mask tokens\"\n",
    "    token_logits = LM(sentence_tkn)[0]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    mask_token_logits = mask_token_logits.flatten()\n",
    "    mask_token_prob = torch.softmax(mask_token_logits,dim=0)\n",
    "    _, top_K_tokens = torch.topk(mask_token_logits, K)\n",
    "    top_K_tokens = top_K_tokens.tolist()    \n",
    "    print('Fill in the blank (\"token : score\") from ' + LM.__class__.__name__ + \": \")\n",
    "    print(\"  \" + sentence)\n",
    "    for ii in range(K):\n",
    "        my_idx = top_K_tokens[ii]\n",
    "        score = str(round(mask_token_prob[my_idx].item(),3))\n",
    "        print(\"  \" + tokenizer.decode([my_idx]) + \" : \" + score)\n",
    "    print(\"\")\n",
    "    for token in top_K_tokens:\n",
    "        print(sentence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained BERT model/tokenizer\n",
    "try:\n",
    "    model_class\n",
    "    print('Cannot load BERT model because...')\n",
    "    print('  A model is already loaded.')\n",
    "except NameError:\n",
    "    model_class = BertModel\n",
    "    LM_class = BertForMaskedLM\n",
    "    tokenizer_class = BertTokenizer\n",
    "    pretrained_weights = 'bert-large-uncased'\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "    model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "    E = model.get_input_embeddings()\n",
    "    NE = E.num_embeddings\n",
    "    LM = LM_class.from_pretrained(pretrained_weights)\n",
    "    encode_word = lambda myword : tokenizer.encode(check_fragment(myword), add_special_tokens=False, return_tensors=\"pt\")\n",
    "    encode_sentence = lambda sentence : tokenizer.encode(check_sentence(sentence), add_special_tokens=True, return_tensors=\"pt\")\n",
    "    encode_sentence_plain = lambda sentence : tokenizer.encode(sentence, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    decode_sentence = lambda tkns : tokenizer.decode(torch.squeeze(tkns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or we can load GPT2 (but not both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT2\n",
    "try:\n",
    "    model_class\n",
    "    print('Cannot load GPT2 model because...')\n",
    "    print('  A model is already loaded.')\n",
    "except NameError:\n",
    "    model_class = GPT2Model\n",
    "    LM_class = GPT2LMHeadModel\n",
    "    tokenizer_class = GPT2Tokenizer\n",
    "    pretrained_weights = 'gpt2-xl'\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "    model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "    E = model.get_input_embeddings()\n",
    "    NE = E.num_embeddings\n",
    "    LM = LM_class.from_pretrained(pretrained_weights)\n",
    "    encode_word = lambda myword : tokenizer.encode(check_fragment(myword), add_special_tokens=False, add_prefix_space=True, return_tensors=\"pt\")\n",
    "    encode_sentence = lambda sentence : tokenizer.encode(check_sentence(sentence), add_special_tokens=True, add_prefix_space=True, return_tensors=\"pt\")\n",
    "    encode_sentence_plain = lambda sentence : tokenizer.encode(sentence, add_special_tokens=False, add_prefix_space=True, return_tensors=\"pt\")\n",
    "    decode_sentence = lambda tkns : tokenizer.decode(torch.squeeze(tkns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some simple santify checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkns = encode_sentence('To be or not to be.')\n",
    "decode_sentence(tkns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only relevant if BERT. GPT2 does not have masks\n",
    "predict_mask('To be or not to [MASK].', K=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gershman phrase similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('data/gershman_phrases.csv')\n",
    "nset = np.max(df['set'])\n",
    "ntype = np.max(df['type'])\n",
    "type_names = ['base','meaning preserve','noun change','preposition change','adjective change']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase(df,idx_set,idx_type):\n",
    "    # Grab the phrase for a particular numerical \"set\" and \"type\"\n",
    "    # return the string of that phrase\n",
    "    df_sel = df.loc[(df['set']==idx_set) & (df['type']==idx_type)]\n",
    "    assert len(df_sel==1)\n",
    "    return df_sel['phrase'].values[0]\n",
    "\n",
    "# get big similarity matix comparing phrases within a set\n",
    "S = np.zeros((nset,ntype))\n",
    "for sid in range(1,nset+1):\n",
    "    base = get_phrase(df,sid,1)\n",
    "    queries = [get_phrase(df,sid,tid) for tid in range(1,ntype+1)] \n",
    "    sims = compute_hidden_cosine(base, queries, use_special=False, extract='mean')\n",
    "    S[sid-1,:] = np.array(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "R = rankdata(S,axis=1) # replace sim with rank: 1 is lowest sim, 5 is highest sim\n",
    "R = ntype-R # reverse the ranking to become 1 for highest, 5 for lowest\n",
    "mean_R = np.mean(R,axis=0)\n",
    "se_R = sem(R,axis=0)\n",
    "labels = type_names[1:] # 'meaning preserve','noun change','preposition change','adjective change']\n",
    "y_pos = [1,4,2,3] # to create order in paper, 'meaning preserve','preposition change','adjective change','noun change']\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "ax.barh(y_pos, mean_R[1:], height=.9, xerr=se_R[1:], color='black', align='center')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xlabel('Average Rank (model)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
